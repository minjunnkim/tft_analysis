{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify CUDA usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFTEnvironment:\n",
    "    def __init__(self):\n",
    "        self.state_size = 50  # Example state size\n",
    "        self.action_size = 10  # Example action size\n",
    "        self.state = np.zeros(self.state_size)\n",
    "        self.episode_length = 200  # Longer episodes\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.zeros(self.state_size)\n",
    "        self.current_step = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = self.calculate_reward(action)\n",
    "        self.state[action] += 1\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.episode_length\n",
    "        return self.state, reward, done\n",
    "\n",
    "    def calculate_reward(self, action):\n",
    "        base_reward = random.random() * (action + 1)\n",
    "        synergy_bonus = self.get_synergy_bonus(action)\n",
    "        placement_penalty = self.get_placement_penalty()\n",
    "        damage_dealt_bonus = self.get_damage_dealt_bonus(action)\n",
    "        survival_time_bonus = self.get_survival_time_bonus()\n",
    "        reward = base_reward + synergy_bonus + damage_dealt_bonus + survival_time_bonus - placement_penalty\n",
    "        return reward\n",
    "\n",
    "    def get_synergy_bonus(self, action):\n",
    "        return random.random() * 2 if self.state[action] >= 2 else 0\n",
    "\n",
    "    def get_placement_penalty(self):\n",
    "        return random.random() * 1.5 if self.current_step < self.episode_length / 2 else 0\n",
    "\n",
    "    def get_damage_dealt_bonus(self, action):\n",
    "        return random.random() * action\n",
    "\n",
    "    def get_survival_time_bonus(self):\n",
    "        return random.random() * (self.episode_length - self.current_step) / self.episode_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN (Deep-Q-Network) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = DQN(state_size, action_size).to(device)\n",
    "        self.target_model = DQN(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.update_target_model()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            act_values = self.model(state)\n",
    "        return torch.argmax(act_values).item()\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "            target = self.model(state).detach().clone()\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                t = self.target_model(next_state).detach()\n",
    "                target[0][action] = reward + self.gamma * torch.max(t)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = nn.MSELoss()(self.model(state), target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_state_dict(torch.load(name))\n",
    "\n",
    "    def save(self, name):\n",
    "        torch.save(self.model.state_dict(), name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment and agent\n",
    "env = TFTEnvironment()\n",
    "agent = DQNAgent(state_size=env.state_size, action_size=env.action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "episodes = 1000\n",
    "batch_size = 32\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    for time in range(env.episode_length):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            agent.update_target_model()\n",
    "            print(f\"Episode {e}/{episodes} finished\")\n",
    "            break\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "agent.save(\"models/dqn_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaNs found in model weights.\n"
     ]
    }
   ],
   "source": [
    "# Load the original PyTorch model\n",
    "original_model = DQN(env.state_size, env.action_size)\n",
    "original_model.load_state_dict(torch.load(\"models/dqn_model.pth\"))\n",
    "\n",
    "def check_for_nans(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if torch.isnan(param).any():\n",
    "            print(f\"NaNs found in {name}\")\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "if check_for_nans(original_model):\n",
    "    print(\"NaNs found in model weights. Reinitializing weights.\")\n",
    "    original_model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
    "else:\n",
    "    print(\"No NaNs found in model weights.\")\n",
    "\n",
    "torch.save(original_model.state_dict(), 'models/dqn_model.pth')\n",
    "\n",
    "original_model.to(device)\n",
    "original_model.eval()\n",
    "\n",
    "# Generate a random input\n",
    "input_data = np.random.random((1, env.state_size)).astype(np.float32)\n",
    "torch_input = torch.tensor(input_data).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the PyTorch model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully saved to models/dqn_model.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch.onnx\n",
    "\n",
    "# Convert the reinitialized model to ONNX format\n",
    "onnx_file_path = 'models/dqn_model.onnx'\n",
    "torch.onnx.export(\n",
    "    original_model, \n",
    "    torch_input, \n",
    "    onnx_file_path, \n",
    "    export_params=True, \n",
    "    opset_version=10, \n",
    "    do_constant_folding=True, \n",
    "    input_names=['input'], \n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    ")\n",
    "\n",
    "print(f\"Model successfully saved to {onnx_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize the ONNX model using TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of network inputs: 1\n",
      "Input 0: input, shape: (-1, 50), dtype: DataType.FLOAT\n",
      "Number of network outputs: 1\n",
      "Output 0: output, shape: (-1, 10), dtype: DataType.FLOAT\n",
      "Model successfully optimized and saved to models/dqn_model.trt\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "def build_engine(onnx_file_path, engine_file_path):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "        config = builder.create_builder_config()\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)  # 1GB\n",
    "\n",
    "        with open(onnx_file_path, 'rb') as model:\n",
    "            if not parser.parse(model.read()):\n",
    "                print('Failed to parse the ONNX file.')\n",
    "                for error in range(parser.num_errors()):\n",
    "                    print(parser.get_error(error))\n",
    "                return None\n",
    "\n",
    "        # Check and print network inputs and outputs\n",
    "        print(f\"Number of network inputs: {network.num_inputs}\")\n",
    "        for i in range(network.num_inputs):\n",
    "            input = network.get_input(i)\n",
    "            print(f\"Input {i}: {input.name}, shape: {input.shape}, dtype: {input.dtype}\")\n",
    "\n",
    "        print(f\"Number of network outputs: {network.num_outputs}\")\n",
    "        for i in range(network.num_outputs):\n",
    "            output = network.get_output(i)\n",
    "            print(f\"Output {i}: {output.name}, shape: {output.shape}, dtype: {output.dtype}\")\n",
    "\n",
    "        # Mark the input and output for optimization profile\n",
    "        input_tensor = network.get_input(0)\n",
    "        profile = builder.create_optimization_profile()\n",
    "        profile.set_shape(input_tensor.name, (1, 50), (16, 50), (32, 50))\n",
    "        config.add_optimization_profile(profile)\n",
    "\n",
    "        # Attempt to build the engine and catch any errors\n",
    "        try:\n",
    "            serialized_engine = builder.build_serialized_network(network, config)\n",
    "            if serialized_engine is None:\n",
    "                print(\"Failed to build the engine.\")\n",
    "                return None\n",
    "\n",
    "            with open(engine_file_path, 'wb') as f:\n",
    "                f.write(serialized_engine)\n",
    "            return serialized_engine\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during engine build: {e}\")\n",
    "            return None\n",
    "\n",
    "onnx_file_path = 'models/dqn_model.onnx'\n",
    "engine_file_path = 'models/dqn_model.trt'\n",
    "\n",
    "engine = build_engine(onnx_file_path, engine_file_path)\n",
    "if engine:\n",
    "    print(f\"Model successfully optimized and saved to {engine_file_path}\")\n",
    "else:\n",
    "    print(\"Failed to optimize the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform inference with the optimized TensorRT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binding 0: input, shape: [1, 50], dtype: <class 'numpy.float32'>\n",
      "Available GPU Memory: 7229931520 bytes, Total GPU Memory: 8585084928 bytes\n",
      "Memory Needed: 200\n",
      "Binding 1: output, shape: [1, 10], dtype: <class 'numpy.float32'>\n",
      "Available GPU Memory: 7229931520 bytes, Total GPU Memory: 8585084928 bytes\n",
      "Memory Needed: 40\n",
      "Input data shape: (1, 50), size: 50, dtype: float32\n",
      "[array([ 0.10805288,  0.06681406,  0.12057865, -0.00891978, -0.10379699,\n",
      "        0.16202234,  0.09430063,  0.08437087,  0.02555529, -0.29642096],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import tensorrt as trt\n",
    "import gc  # Import garbage collector\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "def allocate_buffers(engine, context, batch_size=1):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    bindings = []\n",
    "    stream = cuda.Stream()\n",
    "\n",
    "    for binding in range(engine.num_io_tensors):\n",
    "        name = engine.get_tensor_name(binding)\n",
    "        shape = engine.get_tensor_shape(name)\n",
    "        \n",
    "        # Replace dynamic dimensions with the actual batch size\n",
    "        shape = [batch_size if dim == -1 else dim for dim in shape]\n",
    "        \n",
    "        dtype = trt.nptype(engine.get_tensor_dtype(name))\n",
    "        \n",
    "        print(f\"Binding {binding}: {name}, shape: {shape}, dtype: {dtype}\")\n",
    "        \n",
    "        size = trt.volume(shape)\n",
    "        \n",
    "        # Check available GPU memory\n",
    "        free_mem, total_mem = cuda.mem_get_info()\n",
    "        print(f\"Available GPU Memory: {free_mem} bytes, Total GPU Memory: {total_mem} bytes\")\n",
    "\n",
    "        print(f\"Memory Needed: {size * np.dtype(dtype).itemsize}\")\n",
    "        \n",
    "        # Ensure enough memory is available\n",
    "        if free_mem < size * np.dtype(dtype).itemsize:\n",
    "            raise MemoryError(\"Not enough GPU memory to allocate buffer\")\n",
    "\n",
    "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "        \n",
    "        bindings.append(int(device_mem))\n",
    "        if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n",
    "            inputs.append((host_mem, device_mem))\n",
    "        else:\n",
    "            outputs.append((host_mem, device_mem))\n",
    "    \n",
    "    return inputs, outputs, bindings, stream\n",
    "\n",
    "def do_inference(context, bindings, inputs, outputs, stream, batch_size=1):\n",
    "    # Transfer input data to the GPU\n",
    "    for host_mem, device_mem in inputs:\n",
    "        cuda.memcpy_htod(device_mem, host_mem)\n",
    "    \n",
    "    # Run inference\n",
    "    context.execute_v2(bindings)\n",
    "    \n",
    "    # Transfer predictions back from the GPU\n",
    "    for host_mem, device_mem in outputs:\n",
    "        cuda.memcpy_dtoh(host_mem, device_mem)\n",
    "    \n",
    "    # Synchronize the stream\n",
    "    stream.synchronize()\n",
    "    \n",
    "    return [host_mem for host_mem, device_mem in outputs]\n",
    "\n",
    "# Load the TensorRT engine\n",
    "with open('models/dqn_model.trt', 'rb') as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "    serialized_engine = f.read()\n",
    "    engine = runtime.deserialize_cuda_engine(serialized_engine)\n",
    "\n",
    "context = engine.create_execution_context()\n",
    "\n",
    "# Set the input shape dynamically\n",
    "input_shape = (1, env.state_size)  # Assuming batch size of 1\n",
    "context.set_input_shape('input', input_shape)\n",
    "\n",
    "inputs, outputs, bindings, stream = allocate_buffers(engine, context, batch_size=input_shape[0])\n",
    "\n",
    "# Example input\n",
    "input_data = np.random.random(input_shape).astype(np.float32)\n",
    "inputs[0][0][:] = input_data.ravel()  # Filling the host input buffer\n",
    "\n",
    "# Print the size and type of the input data\n",
    "print(f\"Input data shape: {input_data.shape}, size: {input_data.size}, dtype: {input_data.dtype}\")\n",
    "\n",
    "# Run garbage collection to free up memory\n",
    "gc.collect()\n",
    "\n",
    "# Perform inference\n",
    "output = do_inference(context, bindings, inputs, outputs, stream)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare PyTorch and TensorRT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Throughput: 1062.21 inferences/second\n",
      "TensorRT Throughput: 8430.12 inferences/second\n",
      "PyTorch Memory Usage: Current = 88 bytes, Peak = 728 bytes\n",
      "PyTorch Latency: Mean = 0.000534 seconds, Std = 0.001903 seconds\n",
      "TensorRT Latency: Mean = 0.000114 seconds, Std = 0.001975 seconds\n",
      "PyTorch Power Consumption: 0.000000 watts\n",
      "Mean Absolute Error between PyTorch and TensorRT outputs: 2.5729090339154936e-05\n",
      "Mean Squared Error between PyTorch and TensorRT outputs: 1.0547458462184522e-09\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "# Measure throughput\n",
    "def measure_throughput(model, input_data, device, num_iterations=1000):\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_iterations):\n",
    "            _ = model(input_data)\n",
    "    end_time = time.time()\n",
    "    return num_iterations / (end_time - start_time)\n",
    "\n",
    "def measure_trt_throughput(context, bindings, inputs, outputs, stream, num_iterations=1000):\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_iterations):\n",
    "        _ = do_inference(context, bindings, inputs, outputs, stream)\n",
    "    end_time = time.time()\n",
    "    return num_iterations / (end_time - start_time)\n",
    "\n",
    "# Measure memory usage\n",
    "def measure_memory_usage(model, input_data, device):\n",
    "    tracemalloc.start()\n",
    "    with torch.no_grad():\n",
    "        _ = model(input_data)\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    return current, peak\n",
    "\n",
    "# Measure latency\n",
    "def measure_latency(model, input_data, device, num_iterations=1000):\n",
    "    latencies = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_iterations):\n",
    "            start_time = time.time()\n",
    "            _ = model(input_data)\n",
    "            latencies.append(time.time() - start_time)\n",
    "    return np.mean(latencies), np.std(latencies)\n",
    "\n",
    "def measure_trt_latency(context, bindings, inputs, outputs, stream, num_iterations=1000):\n",
    "    latencies = []\n",
    "    for _ in range(num_iterations):\n",
    "        start_time = time.time()\n",
    "        _ = do_inference(context, bindings, inputs, outputs, stream)\n",
    "        latencies.append(time.time() - start_time)\n",
    "    return np.mean(latencies), np.std(latencies)\n",
    "\n",
    "# Measure power consumption\n",
    "def measure_power_consumption(pid, duration=60):\n",
    "    process = psutil.Process(pid)\n",
    "    start_time = time.time()\n",
    "    start_energy = process.cpu_times().user + process.cpu_times().system\n",
    "    time.sleep(duration)\n",
    "    end_energy = process.cpu_times().user + process.cpu_times().system\n",
    "    end_time = time.time()\n",
    "    power_consumption = (end_energy - start_energy) / (end_time - start_time)\n",
    "    return power_consumption\n",
    "\n",
    "# Measure PyTorch throughput\n",
    "pytorch_throughput = measure_throughput(original_model, torch_input, device)\n",
    "\n",
    "# Measure TensorRT throughput\n",
    "trt_throughput = measure_trt_throughput(context, bindings, inputs, outputs, stream)\n",
    "\n",
    "# Measure memory usage for PyTorch\n",
    "pytorch_memory_current, pytorch_memory_peak = measure_memory_usage(original_model, torch_input, device)\n",
    "\n",
    "# Measure latency for PyTorch\n",
    "pytorch_latency_mean, pytorch_latency_std = measure_latency(original_model, torch_input, device)\n",
    "\n",
    "# Measure latency for TensorRT\n",
    "trt_latency_mean, trt_latency_std = measure_trt_latency(context, bindings, inputs, outputs, stream)\n",
    "\n",
    "# Measure power consumption\n",
    "pid = os.getpid()\n",
    "pytorch_power_consumption = measure_power_consumption(pid)\n",
    "\n",
    "# Compare accuracy\n",
    "def compare_accuracy(pytorch_model, trt_context, trt_bindings, trt_inputs, trt_outputs, trt_stream, input_data, device):\n",
    "    torch_input = torch.tensor(input_data).to(device)\n",
    "    with torch.no_grad():\n",
    "        pytorch_output = pytorch_model(torch_input).cpu().numpy()\n",
    "    trt_inputs[0][0][:] = input_data.ravel()\n",
    "    trt_output = do_inference(trt_context, trt_bindings, trt_inputs, trt_outputs, trt_stream)\n",
    "    return pytorch_output, trt_output[0]\n",
    "\n",
    "pytorch_output, trt_output = compare_accuracy(original_model, context, bindings, inputs, outputs, stream, input_data, device)\n",
    "mae = np.mean(np.abs(pytorch_output - trt_output))\n",
    "mse = np.mean((pytorch_output - trt_output) ** 2)\n",
    "\n",
    "# Print results\n",
    "print(f\"PyTorch Throughput: {pytorch_throughput:.2f} inferences/second\")\n",
    "print(f\"TensorRT Throughput: {trt_throughput:.2f} inferences/second\")\n",
    "print(f\"PyTorch Memory Usage: Current = {pytorch_memory_current} bytes, Peak = {pytorch_memory_peak} bytes\")\n",
    "print(f\"PyTorch Latency: Mean = {pytorch_latency_mean:.6f} seconds, Std = {pytorch_latency_std:.6f} seconds\")\n",
    "print(f\"TensorRT Latency: Mean = {trt_latency_mean:.6f} seconds, Std = {trt_latency_std:.6f} seconds\")\n",
    "print(f\"PyTorch Power Consumption: {pytorch_power_consumption:.6f} watts\")\n",
    "print(f\"Mean Absolute Error between PyTorch and TensorRT outputs: {mae}\")\n",
    "print(f\"Mean Squared Error between PyTorch and TensorRT outputs: {mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
